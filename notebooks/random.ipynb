{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# until the github pip foolbox is updated\n",
    "import sys\n",
    "sys.path.insert(0,'/media/rene/code/foolbox')\n",
    "import foolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from IPython.display import display, HTML\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# Nicer way to import the module?\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from models.cifar import resnet\n",
    "from utils.display import load_image, show_img\n",
    "from utils.loading import load_net_cifar\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd.gradcheck import zero_gradients\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import resnet18, vgg16\n",
    "import foolbox\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Adversarial Dataset\n",
    "* Images from FGSM look resonable\n",
    "* For some reason about 1% of the images didn't work and are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model_file resnet-50\n"
     ]
    }
   ],
   "source": [
    "with torch.cuda.device(1):\n",
    "    with open('/media/rene/data/adv_denoising/cifar10/adv_fgsm/resnet50/sample/files_df_adv.pkl', 'rb') as f:\n",
    "        files_df = pickle.load(f)\n",
    "\n",
    "    model_loc = '/media/rene/data/adv_denoising/cifar10/cifar10_normal/models/resnet-50_model_best.pth.tar'\n",
    "    classifier_model = load_net_cifar(model_loc).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/rene/data/adv_denoising/cifar10/cifar10_normal/train/train_21959.png\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-fcd6ba785211>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     fmodel = foolbox.models.PyTorchModel(\n\u001b[0;32m---> 16\u001b[0;31m         classifier_model, bounds=(0, 1), num_classes=10, preprocessing=(mean, std))\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/rene/code/foolbox/foolbox/models/pytorch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, bounds, num_classes, channel_axis, device, preprocessing)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/rene/ADV/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/rene/ADV/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/rene/ADV/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0;31m# Tensors stored in modules are graph leaves, and we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/rene/ADV/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "with torch.cuda.device(1):\n",
    "    mean = np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))\n",
    "    std = np.array([0.2023, 0.1994, 0.2010]).reshape((3, 1, 1))\n",
    "\n",
    "    idx = np.random.randint(0, len(files_df['train']))\n",
    "\n",
    "    img_path = files_df['train']['path'].iloc[idx]\n",
    "    print(img_path)\n",
    "    adv_path = files_df['train']['adv_path'].iloc[idx]\n",
    "    label = files_df['train']['class'].iloc[idx]\n",
    "\n",
    "    image = load_image(img_path)\n",
    "    adversarial = load_image(adv_path)\n",
    "\n",
    "    fmodel = foolbox.models.PyTorchModel(\n",
    "        classifier_model, bounds=(0, 1), num_classes=10, preprocessing=(mean, std))\n",
    "\n",
    "    print('label', label)\n",
    "    print('predicted class', np.argmax(fmodel.predictions(image)))\n",
    "    print('adversarial class', np.argmax(fmodel.predictions(adversarial)))\n",
    "\n",
    "    show_img(image, adversarial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we have layers where at each location only one feature can be active. Then do some kind of beam search over this to the the overall activations, just like in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model_file resnet-50\n"
     ]
    }
   ],
   "source": [
    "from models.cifar import resnet\n",
    "from utils.data import make_generators_DF_cifar\n",
    "from utils.train_val_denoise import train_epoch_denoise, validate_epoc_denoise, save_checkpoint\n",
    "from utils.loading import load_net_cifar\n",
    "from models import DenoiseNet, DenoiseHGD, DenoiseLoss, UNet   \n",
    "\n",
    "files_df_loc = '/media/rene/data/adv_denoising/cifar10/adv_fgsm/resnet50/sample/files_df_adv.pkl'\n",
    "model_loc = '/media/rene/data/adv_denoising/cifar10/cifar10_normal/models/resnet-50_model_best.pth.tar'\n",
    "device = torch.device('cuda:1')\n",
    "batch_size = 128\n",
    "num_workers = 4\n",
    "\n",
    "with open(files_df_loc, 'rb') as f:\n",
    "    files_df = pickle.load(f)\n",
    "    \n",
    "dataloaders = make_generators_DF_cifar(files_df, batch_size, num_workers, size=32, \n",
    "                                       path_colname='path', adv_path_colname='adv_path', return_loc=False)\n",
    "\n",
    "model_name = 'DenoiseHGD'\n",
    "fwd_out = [64, 128, 256, 256, 256]\n",
    "num_fwd = [2, 3, 3, 3, 3]\n",
    "back_out = [64, 128, 256, 256]\n",
    "num_back = [2, 3, 3, 3]\n",
    "fwd_in = 3\n",
    "denoiser = DenoiseHGD(32, 32, fwd_in, fwd_out, num_fwd, back_out, num_back).to(device)\n",
    "classifier = load_net_cifar(model_loc).to(device)\n",
    "loss = DenoiseLoss(n=1, hard_mining=0, norm=False)\n",
    "model = DenoiseNet(classifer=classifier, denoiser=denoiser, loss=loss).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/rene/ADV/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:122: UserWarning: nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/media/rene/ADV/lib/python3.6/site-packages/torch/nn/functional.py:1961: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Acc 93.266 \t Adversarial Acc  34.774 Loss (213.4918)\tTime (0.125)\t Data Time (0.030)\n"
     ]
    }
   ],
   "source": [
    "val_adv_acc, val_loss = validate_epoc_denoise(dataloaders['val'], model, requires_control=False, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n",
      "torch.Size([1, 3, 16, 32, 2])\n",
      "torch.Size([1, 3, 16, 16, 2, 2])\n",
      "torch.Size([1, 3, 16, 16, 4])\n",
      "torch.Size([768, 4])\n",
      "x.mean() tensor(0.6594)\n",
      "x.mean() tensor(0.6830)\n",
      "torch.Size([1, 3, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "kernel_size = (2, 2)\n",
    "stride = (2, 2)\n",
    "\n",
    "img = load_image('/media/rene/data/adv_denoising/cifar10/cifar10_normal/train/train_20769.png')\n",
    "x = torch.from_numpy(img).unsqueeze(0)\n",
    "init_size = x.shape\n",
    "print(init_size)\n",
    "x = x.unfold(2, kernel_size[0], stride[0])\n",
    "print(x.shape)\n",
    "x = x.unfold(3, kernel_size[1], stride[1])\n",
    "print(x.shape)\n",
    "x = x.contiguous().view(x.size()[:4] + (-1,))\n",
    "print(x.shape)\n",
    "x = x.view(-1, 4)\n",
    "print(x.shape)\n",
    "\n",
    "print('x.mean()', x.mean())\n",
    "x = torch.stack([\n",
    "                 x_i[torch.multinomial(x_i, num_samples=1)] for i, x_i in enumerate(torch.unbind(x, dim=0), 0)\n",
    "                ], dim=0)\n",
    "print('x.mean()', x.mean())\n",
    "x = x.contiguous().view(init_size[0], init_size[1], int(init_size[2]/2), int(init_size[3]/2))\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 64, 32, 32])\n",
      "torch.Size([5, 64, 16, 16, 2, 2])\n",
      "torch.Size([81920, 4])\n",
      "x.mean() tensor(10.0000, device='cuda:1', dtype=torch.float64)\n",
      "x.mean() tensor(10.0000, device='cuda:1', dtype=torch.float64)\n",
      "torch.Size([5, 64, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "kernel_size = (2, 2)\n",
    "stride = (2, 2)\n",
    "\n",
    "x = torch.from_numpy(np.reshape(np.random.normal(10, .001, 5*64*32*32), (5, 64, 32, 32))).to(device)\n",
    "init_size = x.shape\n",
    "print(init_size)\n",
    "x = x.unfold(2, kernel_size[0], stride[0]).unfold(3, kernel_size[1], stride[1])\n",
    "print(x.shape)\n",
    "x = x.contiguous().view(-1, 4)\n",
    "print(x.shape)\n",
    "print('x.mean()', x.mean())\n",
    "x = torch.stack([\n",
    "                 x_i[torch.multinomial(x_i, num_samples=1)] for i, x_i in enumerate(torch.unbind(x, dim=0), 0)\n",
    "                ], dim=0)\n",
    "print('x.mean()', x.mean())\n",
    "x = x.contiguous().view(init_size[0], init_size[1], int(init_size[2]/2), int(init_size[3]/2))\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "torch.Size([2, 3, 32, 32])\n",
      "torch.Size([2, 3, 16, 16, 2, 2])\n",
      "torch.Size([1536, 4])\n",
      "torch.Size([2, 3, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "img1 = np.expand_dims(load_image('/media/rene/data/adv_denoising/cifar10/cifar10_normal/train/train_20769.png'),0)\n",
    "img2 = np.expand_dims(load_image('/media/rene/data/adv_denoising/cifar10/cifar10_normal/train/train_317.png'),0)\n",
    "\n",
    "img = np.concatenate((img1, img2), axis=0)\n",
    "x = torch.from_numpy(img)\n",
    "\n",
    "print(device)\n",
    "kernel_size = (2, 2)\n",
    "stride = (2, 2)\n",
    "init_size = x.shape\n",
    "\n",
    "with torch.cuda.device(1):\n",
    "    print(x.shape)\n",
    "    x = x.unfold(2, kernel_size[0], stride[0]).unfold(3, kernel_size[1], stride[1])\n",
    "    print(x.shape)\n",
    "    x = x.contiguous().view(-1, 4)\n",
    "    print(x.shape)\n",
    "    x = torch.stack([\n",
    "             x_i[torch.multinomial(x_i, num_samples=1)] for i, x_i in enumerate(torch.unbind(x, dim=0), 0)\n",
    "            ], dim=0)\n",
    "    x = x.contiguous().view(init_size[0], init_size[1], int(init_size[2]/2), int(init_size[3]/2))\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample randomly if all equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "torch.Size([1536, 4])\n",
      "torch.Size([2, 3, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "img1 = np.expand_dims(load_image('/media/rene/data/adv_denoising/cifar10/cifar10_normal/train/train_20769.png'),0)\n",
    "img2 = np.expand_dims(load_image('/media/rene/data/adv_denoising/cifar10/cifar10_normal/train/train_317.png'),0)\n",
    "\n",
    "img = np.concatenate((img1, img2), axis=0)\n",
    "x = torch.from_numpy(img)\n",
    "\n",
    "print(device)\n",
    "kernel_size = (2, 2)\n",
    "stride = (2, 2)\n",
    "init_size = x.shape\n",
    "\n",
    "with torch.cuda.device(1):\n",
    "    def gen_random(values):\n",
    "        if torch.sum(values) == 0:\n",
    "            idx = torch.multinomial(values, num_samples=1)\n",
    "        else:\n",
    "            idx = int(torch.randint(0, values.shape[0], size=(1,))[0])\n",
    "            \n",
    "        return values[idx]\n",
    "    x = x.unfold(2, kernel_size[0], stride[0]).unfold(3, kernel_size[1], stride[1])\n",
    "    x = x.contiguous().view(-1, 4)\n",
    "    print(x.shape)\n",
    "    x = torch.stack([\n",
    "             gen_random(x_i) for i, x_i in enumerate(torch.unbind(x, dim=0), 0)\n",
    "            ], dim=0)\n",
    "    x = x.contiguous().view(init_size[0], init_size[1], int(init_size[2]/2), int(init_size[3]/2))\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Too slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "torch.Size([1536, 4])\n",
      "tensor([[0.9961, 0.8941, 0.9725, 0.5451],\n",
      "        [0.4902, 0.3961, 0.1804, 0.2980],\n",
      "        [0.8078, 0.8863, 0.3059, 0.3843],\n",
      "        ...,\n",
      "        [0.0941, 0.1843, 0.1373, 0.1255],\n",
      "        [0.2118, 0.2471, 0.1765, 0.2000],\n",
      "        [0.2784, 0.2118, 0.1922, 0.1647]])\n",
      "tensor([0, 1, 0,  ..., 3, 3, 0])\n",
      "torch.Size([1536])\n",
      "torch.Size([1536, 1536])\n",
      "tensor([[0.9961, 0.8941, 0.9961,  ..., 0.5451, 0.5451, 0.9961],\n",
      "        [0.4902, 0.3961, 0.4902,  ..., 0.2980, 0.2980, 0.4902],\n",
      "        [0.8078, 0.8863, 0.8078,  ..., 0.3843, 0.3843, 0.8078],\n",
      "        ...,\n",
      "        [0.0941, 0.1843, 0.0941,  ..., 0.1255, 0.1255, 0.0941],\n",
      "        [0.2118, 0.2471, 0.2118,  ..., 0.2000, 0.2000, 0.2118],\n",
      "        [0.2784, 0.2118, 0.2784,  ..., 0.1647, 0.1647, 0.2784]])\n"
     ]
    }
   ],
   "source": [
    "img1 = np.expand_dims(load_image('/media/rene/data/adv_denoising/cifar10/cifar10_normal/train/train_20769.png'),0)\n",
    "img2 = np.expand_dims(load_image('/media/rene/data/adv_denoising/cifar10/cifar10_normal/train/train_317.png'),0)\n",
    "\n",
    "img = np.concatenate((img1, img2), axis=0)\n",
    "x = torch.from_numpy(img)\n",
    "\n",
    "print(device)\n",
    "kernel_size = (2, 2)\n",
    "stride = (2, 2)\n",
    "init_size = x.shape\n",
    "\n",
    "with torch.cuda.device(1):\n",
    "    x = x.unfold(2, kernel_size[0], stride[0]).unfold(3, kernel_size[1], stride[1])\n",
    "    x = x.contiguous().view(-1, 4)\n",
    "    print(x.shape)\n",
    "    print(x)\n",
    "    idx = torch.randint(0, x.shape[1], size=(x.shape[0],)).type(torch.LongTensor)\n",
    "    print(idx)\n",
    "    x = torch.index_select(x, 1, idx)\n",
    "    print(idx.shape)\n",
    "    print(x.shape)\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADV",
   "language": "python",
   "name": "adv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
